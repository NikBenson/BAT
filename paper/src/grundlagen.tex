In diesem Kapitel werden die grundsätzlichen Definitionen und eine historische einordnung sowie ein Überblich über die aktuellen Möglichkeiten von \ac{DSL} geschaffen.

\subsection{\aclp{DSL}}\label{subsec:domain-specific-languages}
Die erste Frage, die es zu klären gibt, ist: Was sind eigentlich \ac{DSL}?
\begin{displayquote}[\cite{jetbrains-sro-no-dateC}]
    A \ac{DSL} is a programming language with a higher level of abstraction optimized for a specific class of problems.
    A \ac{DSL} uses the concepts and rules from the field or domain.
\end{displayquote}
Laut JetBrains definieren sich \acp{DSL} also über ihr Sprachdesign, welches Konzepte einer spezifischen, fachlichen Domäne beinhaltet.
Wikipedia definiert es etwas anders:
\begin{displayquote}[\cite{wikipedia-contributors-2024C}]
    A \ac{DSL} is a computer language specialized to a particular application domain.
    This is in contrast to a \ac{GPL}, which is broadly applicable across domains.
\end{displayquote}
Hier werden \acp{DSL} als gegenteil von \acp{GPL} genannt, welche zusammen die Programmiersprachen ausmachen.
Diese sind im Wörterbuch definiert als:
\begin{displayquote}[\cite{unknown-author-no-date}]
    Code of reserved words and symbols used in computer programs, which give instructions to the computer on how to accomplish certain computing tasks.
\end{displayquote}
Also eine Kodierung, welche Anweisungen an einen Computer gibt.

Zusammenfassend lässt sich sagen, dass \acp{DSL} Kodierungen sind, die fachspezifisch von einer Menge von Schlüsselwörtern und strukturierten, dynamischen Inhalt auf eine von (mindestens) einem Computer--(Programm) verstandene Eingabe--(Datei) abbildet.
Dabei stehen fachspezifische Paradigmen vor den klassischen Programmierparadigmen~\autocite{wikipedia-contributors-2024D} im Zentrum des Sprachdesigns.

Das wohl bekannteste Beispiel ist \ac{SQL}.~\autocite{unknown-author-2023}
\ac{SQL} ist eine durch die \ac{ISO} genormte \ac{DSL}; spezifischer eine sogenannte \textit{query language}.
Auf Deutsch Abfragesprachen genannt, dienen diese \acp{DSL}, der interaktionen mit \acp{DBMS}.
Die Domäne ist in diesem Fall also die Datenhaltung \textit{strukturierter} Daten.

Da es etliche \acp{DBMS} gibt, welche dem \ac{SQL}--Standard folgen, gibt es für \ac{SQL} auch etliche Compiler--Pipelines beziehungsweise Interpreter.
Diese Compiler--Pipelines sind dabei stark mit dem jeweiligen \ac{DBMS} verwoben.
Es gibt also keine einheitliche Codebasis.
Dies wäre mit bekannten Tools des Compilerbaus, namentlich \textbf{Lex} \& \textbf{YACC}.
Diese werden im folgenden Kapitel behandelt.

\begin{figure}[ht]
    \begin{subfigure}[c]{0.5\textwidth}
        \begin{center}
            \includegraphics[height=2.5cm]{../assets/img/diagrams/compiler.mmd}
        \end{center}
        \caption{Compiler}
        \label{subfig:compiler-and-interpreter-compiler}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \begin{center}
            \includegraphics[height=2.5cm]{../assets/img/diagrams/interpreter.mmd}
        \end{center}
        \caption{Interpreter}
        \label{subfig:compiler-and-interpreter-interpreter}
    \end{subfigure}
    \caption{Compiler \& Interpreter~\autocite{aho-2006}}
    \label{fig:compiler-and-interpreter}
\end{figure}
\newpage

\subsection{Compilerbau}\label{subsec:compilerbau}
\begin{wrapfigure}{rH}{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.47\textwidth]{../assets/img/diagrams/compiler_phases.mmd}
    \end{center}
    \caption{Phasen der Compiler--Pipeline~\autocite{aho-2006}}
    \label{fig:compiler-phases}
\end{wrapfigure}
Was von Entwicklern umgangssprachlich \enquote{Compiler} genannt wird, bezeichnet eigentlich die Compiler--Pipeline.
Der eigentliche Compiler, oder auch Code Generator, (Siehe \autoref{fig:compiler-phases}) ist ein Tool, welches aus einer abstrakten Representation die Zielsprache, im folgenden Programm genannt, generiert.

Ein Programm ist dabei definiert als eine \enquote{Black--Box}, welche eine Eingabe in eine Ausgabe transformiert.
Damit kann zwischen zwei verschiedenen Aufbauten unterscheiden: Compiler erstellen aus Code ein Programm, welches eine Eingabe in eine Ausgabe transformiert (Siehe \autoref{subfig:compiler-and-interpreter-compiler}) und Interpreter transformieren unter der Verwendung von Code eine Eingabe in eine Ausgabe.~(Siehe \autoref{subfig:compiler-and-interpreter-interpreter})

Effektiv benötigen beide Klassen von Tools die selben Phasen.
Es muss auf irgend einem Weg, durch das konsekutive ausführen von Transformationen aus dem Code das Programm werden.

Diese Phasen sind klassischer Weise jene aus \autoref{fig:compiler-phases}.
Diese Phasen wurden auch im \ac{POSIX}, einem Standard für Betriebssysteme, festgehalten.~\autocite{ieee-sa-1993}

\paragraph{Lexical Analyzer,} oder auch \textbf{Tokenizer}, gruppieren den Code in \textbf{Tokens}.
Ein Token ist dabei ein Zweitupel aus seinem Typen und seinem Wert, ein optionalen Eintrag in der SymbolTable.

Die zugrundeliegende Datenstruktur bleibt dabei ein Stream.
Es handelt sich also immer noch um eine lineare Datenstruktur.

Der wohl bekannteste Tokenizer ist dabei Lex.~\autocite{wikipedia-contributors-2024G}
Lex folgt dem \ac{POSIX} Standard und ist in \autoref{subsubsec:lex} beschriebene.

\paragraph{Syntax Analyzer,} oder auch \textbf{Parser}, analysieren die Reihenfolge, in der die einzelnen Tokens evaluiert werden.
Dazu wird mittels einer \ac{CFG}~\autocite{wikipedia-contributors-2024H} ein \textbf{syntax tree}, oder auch \textbf{\ac{AST}}, erstellt.

Der \ac{AST} beinhaltet im Regelfall die Tokens, die vom Tokenizer erstellt wurden, als Leafs.
Die Semantische bedeutung des \ac{AST} ist entsprechend die konsekutive Gruppierung von Tokens und anderen Gruppen.

Es werden verschiedene Arten von Parsern eingesetzt, die sich in ihrer Herangehensweise an die Analyse der Eingabesequenz unterscheiden.
Zwei wichtige Kategorien sind Top--Down-- und Bottom--Up--Parser.
\begin{figure}[ht]
    \begin{subfigure}[c]{0.5\textwidth}
        \begin{center}
            \includegraphics[width=\textwidth]{../assets/img/diagrams/bottom_up_parser.mmd}
        \end{center}
        \caption{Bottom--Up--Parser}
        \label{subfig:bottom-up-parser}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \begin{center}
            \includegraphics[width=\textwidth]{../assets/img/diagrams/top_down_parser.mmd}
        \end{center}
        \caption{Top--Down--Parser}
        \label{subfig:top-down-parser}
    \end{subfigure}
    \caption{Arten von Parsern}
    \label{fig:parser-types}
\end{figure}

\subparagraph{Top--Down--Parser} (Siehe \autoref{subfig:top-down-parser}) starten, indem sie die größte Struktur der Grammatik versuchen, anzuwenden.~\autocite{geeksforgeeks-2021A}
Diese wird dann recursive immer weiter zerteilt.

Dabei liest er von links nach rechts.
Dadurch kann es schwierig werden, einen sinnvollen \ac{AST} zu erzeugen, wenn es uneindeutige Abfolgen gibt.

\subparagraph{Bottom--Up--Parser} (Siehe \autoref{subfig:bottom-up-parser}) starten, mit den Terminalen der Grammatik.~\autocite{geeksforgeeks-2021B}
Diese werden dann nach und nach zu größeren Strukturen zusammengesetzt.

Dabei liest er von rechts nach links.
Dadurch kann es schwierig sein, den nächsten Token auszuwählen.

\paragraph{Semantic Analyzer} analysiert die Nodes des \ac{AST} auf semantische korrektheit.
Hier werden zum Beispiel Variable auf Scopes geprüft und das Typechecking durchgeführt.
Auch werden Konstante werte errechnet.

Kurzum ist diese Phase dafür verantwortlich, Feedback zum Code zu geben.
Dabei werden in der Regel Warnings und Errors ausgegeben.

Dies stellt sicher, dass nur korrekte Programme weiter kompiliert werden.
Ausnahmen dazu gibt es zum Beispiel mit JavaScript.
Hier wird jedes Statement versucht zu interpretieren, um einfache Fehler unter den Tisch zu kehren.

\paragraph{Intermediate Code Generator} erstellt Text aus dem \ac{AST} eine Computer--verständliche Zwischendarstellung.

Ein Beispiel aus~\cite{aho-2006} wäre der three--address code.
Dieser ist eine klassische \textbf{intermediate representation} beim Kompilieren zu Assembly.

Der Zweck dieser Zwischendarstellung ist meistens, dass sie in einer Textdatei abgelegt werden kann.
Der Vorteil davon ist, dass das Frontend der Compiler--Pipeline\footnote{Als Frontend des Compilers werden die ersten 5 Phasen bezeichnet, als Backend die letzten beiden.} klassischerweise unabhängig von der Prozessorarchitektur ist und es so potenziell mehrere, unabhängig voneinander entwickelte, Backends für die verschiedenen Prozessorarchitekturen geben kann.

Da \acp{DSL} im Gegensatz zu \acp{GPL} meist keine Abhängigkeit zu Prozessorarchitekturen haben, fällt die Wahl der intermediate representation häufig direkt auf den \ac{AST}.

\paragraph{Machine--Independent Code Optimizer} tun genau das, was der Name auch sagt: sie nehmen mögliche Optimierungen an der intermediate representation vor.

Diese Optimierungen beinhalten zum Beispiel das entfernen von unnötigem Code, Obfuscating von Identifiers, Aufräumen von Ressourcen, \dots.

Der Code kann anhand von verschiedensten Metriken optimiert werden.
Dazu zählt Geschwindigkeit, Dateigröße\footnote{Ist im Web wichtig.}, \dots.

Optimierungsphasen sind immer optional.
Im C--Compiler der \ac{GCC} kann zum Beispiel auch eingestellt werden, wie stark der Code optimiert werden soll.~\autocite{gnu-project-no-date}

\paragraph{Code Generator,} oder auch \textbf{Compiler}, erzeugen nun aus der \textbf{intermediate representation} den \textbf{target--machine code}.

Im Gegensatz zu den meisten anderen Schritten ist diese Phase nicht so standardisiert.
Dies liegt an der möglichen abhängigkeit von Prozessorarchitekturen.

Tools wie \ac{LLVM}~\autocite{llvm-project-2024} nehmen sich der Verallgemeinerung der meisten Prozessorarchitekturen an.
Damit sind sie allerdings relevanter beim Entwickeln von \ac{GPL} als \ac{DSL}.

Tools speziell für \ac{DSL} konnten wir nicht finden.
Allerdings ist diese letzte transformation, wie die Umsetzung aus \autoref{lst:compiler} zeigt, nach entsprechender Aufbereitung der Daten durch die restliche Pipeline, nicht explizit kompliziert.

\paragraph{Machine--Dependent Code Optimizer} optimieren den Code analog zu Machine--Independent Code Optimizer, nur spezifisch für die Zielplattform.

\subsubsection{Lex}\label{subsubsec:lex}
Lex ist die Abkürzung für Lexical Analyzer und das bekannteste Tool zur erstellung von Tokenizern.

Die Tokens werden dabei durch \acp{RegEx} identifiziert und von links nach rechts eingelesen.
Standardmäßig sucht Lex nach dem längsten Treffer, es kann allerdings auch über eine Flag des \ac{CLI} nach dem kürzesten Match gesucht werden.

Lex Programme sind dabei in drei Abschnitte unterteilt:
\lstinputlisting[label={lst:lex-structure},caption={Aufbau eines Lex Programms},language=lex]{../assets/code/c/lex_structure.l}

\paragraph{Declarations} beinhalten vor allem die Namen der Tokens.
Aber auch gegebenenfalls Optionen zur konfiguration des Lex Compilers oder auch C Code, meistens Imports.

Wird Lex zusammen mit \ac{YACC} verwendet, so werden die Namen der Tokens in YACC definiert.
In dem Fall wird hier nur der \ac{YACC} Header importiert und der Abschnitt bleibt ansonsten weitestgehend leer.

\paragraph{Rules} spezifizieren die \ac{RegEx} Ausdrücke und die zugehörigen Tokens, also die Namen aus der ersten Sektion und gegebenenfalls anfallende Einträge in der Symbol Table.
In \autoref{lst:lex-rule} ist beispielhaft eine solche Regel für einen Ganzzahlwert dargestellt, wie sie im Beispiel aus \autoref{sec:vergleich-der-tool-sets-anhand-eines-einfachen-beispiels} verwendet wird.
\begin{lstlisting}[label={lst:lex-rule},caption={Lex Regel},language=lex]
[0-9]+ { yylval.i = atoi(yytext); return INT; }
\end{lstlisting}
\begin{itemize}
    \item \verb|[0-9]+| legt fest, dass eine beliebig lange Folge von Digits erkannt wird.
    \item In den geschweiften Klammern kann zunächst beliebiger C--Code stehen.
    Dieser ist optional.
    Wird anstelle der Klammern ein Semicolon verwendet, so wird das Match ignoriert.
    Dies wird meistens für Whitespace verwendet.
    \begin{itemize}
        \item \verb|yylval| ist eine Variable, die einen eintrag in der Symbol Table repräsentiert.
        Der zugewiesene Wert wird automatisch dem Token zugeordnet, sodass in \ac{YACC} später darauf zugegriffen werden kann.
        Standardmäßig ist \verb|yylval| ein integer, hier ist es ein Union--Type.
        Dies kann in \ac{YACC} eingestellt werden.
        \item \verb|yytext| ist das \acs{RegEx}--Match als String.
        \item \verb|INT| ist der zuvor deklarierte Name des Tokens.
    \end{itemize}
\end{itemize}

Es wird also einem \ac{RegEx} Match eine Funktion zugeordnet.
Diese kann theoretisch alles machen, muss aber den namen des Tokens benennen.
Im regelfall werden allerdings höchstens zwei Statements analog zu \autoref{lst:lex-rule} verwendet.

\paragraph{Auxiliary Functions} sind einfach beliebige C funktionen.
Diese werden allerdings nicht über einen Header exposed, sondern sind nur zur Verwendung innerhalb des Lex Programms gedacht.

Soll ein Lex Programm ein eigenes \ac{CLI} definieren, so wird hier üblicherweise die \verb|main()| Methode definiert.
Anderenfalls bleibt dieser Abschnitt zumeist leer, da Funktionen ausgelagert werden.
In dem Fall werden sie im ersten Abschnitt importiert.

\paragraph*{}
Das ergebnis des Lex Compilers ist immer eine C--Datei.
Diese kann entweder von anderen C--Programmen verwendet werden, oder mittels eines herkömmlichen C--Compilers kompiliert werden.

Meistens wird Lex zusammen mit \ac{YACC} verwendet.

\subsubsection{\acs{YACC}}
\ac{YACC} ist ein Tool zum erstellen von \ac{LALR} Parsern nach dem \ac{POSIX} Standard.

\paragraph{\acs{LALR}--Parser} sind dabei eine Untergruppe der Bottom--Up--Parser.
Besonders beliebt sind \acs{LALR}--Parser in der Praxis, da sie besonders effizient und robust sind, obwohl sie alle \ac{CFG} parsen können.
Außerdem ermöglicht der Lookahead es, mehrdeutige Grammatiken zu parsen.

Einige Alternativen sind:

\subparagraph{\ac{LL} Parser,} diese sind schneller, bilden allerdings nicht alle \ac{CFG} ab.

\subparagraph{\ac{LR} Parser,} spezifischer LR(1) Parser, auch genannt canonical \acs{LR} Parser.
Diese sind die mächtigsten Parser in der Familie der \ac{LR} parser.
Allerdings benötigen sie mehr Speicherplatz und sind schwierig zu konstruieren.

\paragraph*{}
Ein \ac{YACC} Programm ist immer in die folgenden 3 Teile gegliedert:
\lstinputlisting[label={lst:yacc-structure},caption={Aufbau eines \acs{YACC} Programms},language=yacc]{../assets/code/c/yacc_structure.y}

\paragraph{Definitions} beinhalten unter anderem die definition der Tokens.
Diese werden dann auch von Lex übernommen.

Im Kontext von Parsern werden die Namen der Tokens als Terminale bezeichnet.
Dies ergibt sich aus der definition der \ac{CFG} als 4--Tupel $G=(V,\Sigma,R,S)$.~\autocite{sipser-1997}
\begin{itemize}
    \item[$\Sigma$] repräsentiert die Terminale.
    Terminale sind die \enquote{Wörter} der Grammatik.
    Also die atomaren bausteine eines \enquote{Satzes}.
    \item[$V$] sind die Nicht--Terminale.
    Nicht--Terminale können als Variable verstanden werden.
    \item[$R$] sind die Regeln.
    Sie beschreiben, wie ein Nicht--Terminal in andere Terminale und Nicht--Terminale zerteilt werden kann.
    \item[$S$] ist das startsymbol.
    Es ist immer ein Element aus $V$ oder $R$ und beschreibt, was die Grammatik abbilden will.
\end{itemize}
Die Regeln und Nicht--Terminale werden im zweiten Abschnitt des Programms definiert, allerdings werden die anderen beiden im ersten festgelegt.

Neben $S$ und $\Sigma$ kann hier auch der Aufbau der Symbol Table beschrieben werden.
Es wird festgelegt, welche Typen ihr zugewiesen werden können.
Im Regelfall ist dies ein Union Type.
In dem Fall muss für alle Terminale und Nicht--Terminale definiert werden, welchen Typ sie speichern wollen.

\paragraph{Rules} spezifizieren die Nicht--Terminale direkt zusammen mit den Ableitungsregeln.
\begin{lstlisting}[label={lst:yacc-rule},caption={\acs{YACC} Regel},language=yacc]
expression: INT { $$ = $1; }
          | INT PLUS expression { $$ = $1 + $3; }
          ;
\end{lstlisting}
\autoref{lst:yacc-rule} ist dazu ein komplett fiktives Beispiel ohne Zusammenhang.

Es wird das Nicht--Terminal \verb|expression| definiert.
Dieses lässt sich konstruieren aus den beiden Konstruktionsregeln ($R$):
\begin{itemize}
    \item Terminal \verb|INT|
    \item oder der Abfolge Terminal \verb|INT|, Terminal \verb|PLUS|, Nicht--Terminal \verb|expression|.
\end{itemize}
Es ist also rekursiv definiert.

In den geschweiften Klammern kann wieder beliebiger C--Code stehen.
Allerdings wird hier im Regelfall nur die Symbol Table manipuliert.
\verb|$$| repräsentiert dabei den Eintrag des gebildeten Nicht--Terminals (hier \verb|expression|) und \verb|$i| den Eintrag des $i$--ten Kinds.

\paragraph{Auxiliary Functions} sind analog zu Lex auch einfache C--Funktionen.

\paragraph*{}
\ac{YACC} erstellt dabei nicht automatisch einen \ac{AST}.
Um eine Sinnvolle Ausgabe zu erhalten muss der Entwickler selbst eine Datenstruktur erzeugen.

Im Regelfall wird dies dadurch gelöst, dass die Nodes des Baumes teil des Union--Types der Symbol Table sind.
Alternativ können die Nodes auch anhand von Integer--IDs identifiziert werden.
So macht es auch das Beispiel aus \autoref{sec:vergleich-der-tool-sets-anhand-eines-einfachen-beispiels}.
\begin{lstlisting}[label={lst:yacc-ast},caption={\acs{YACC} \acs{AST}},language=yacc]
answer_1: question_1 t_bool { $$ = node(t_ELEMENT, 2, $1, node(t_VALUE, 1, $2)); };
t_bool: BOOL { $$ = leaf(t_BOOL, (YYSTYPE) $1); };
\end{lstlisting}
Analog zu \autoref{lst:yacc-ast} kann so der \ac{AST} leicht konstruiert werden.

\subsubsection{Compiler}
Auch zum erstellen von Compilern gibt es Tools, allerdings ist es hier auch üblich, diese manuel zu erstellen.
In \autoref{lst:compiler} ist beispielhaft ein solcher Compiler für den in \autoref{sec:vergleich-der-tool-sets-anhand-eines-einfachen-beispiels} verwendeten Compiler der PlayerConfig--Sprache abgebildet.
\lstinputlisting[label={lst:compiler},caption={Einfacher Compiler in C},language=c]{../assets/code/c/compiler.c}
Da in diesem Fall die intermediate representation identisch zum \ac{AST} ist, muss hier lediglich der Baum traversiert werden.
Sind diese beiden Modelle nicht identisch, muss das Modell der intermediate representation traversiert werden.
In jedem fall bleibt es allerdings bei einer traversierung.

In diesem Fall wird über eine rekursive Funktion und ein Switch--Statement das \acs{JSON}--Objekt ausgegeben.

Daraus ist ersichtlich, dass bei entsprechend guter Aufbereitung der Daten in den vorangegangenen Phasen der Compiler--Pipeline, das manuelle erstellen eines Compilers keine nicht zu bewältigende Aufgabe ist.

\subsection{\acl{MPS}}\label{subsec:meta-programming-system}
\ac{MPS} macht einiges anders als \ac{POSIX}.
Der Ausgangspunkt\footnote{Eingabe des Programms} ist keine Textdatei, sondern der \ac{AST} der zu definierenden Sprache.
Dies funktioniert, da \ac{MPS} nur in der gleichnamigen \ac{IDE} verwendet werden kann.

Der \ac{AST} wird dabei durch den \textbf{Editor} in eine Representation für den Menschen gerendert und über Transformationsregeln bearbeitet.

Auch das Kompilieren läuft in der Regel anders.
Einfache Sprachen haben klassische Code Generatoren, in \ac{MPS} \textbf{TextGen} genannt.
Im Regelfall werden allerdings Transpiler\footnote{Ein Compiler, der eine Sprache als Eingabe und eine andere Sprache als Ausgabe hat.} verwendet.
Diese übersetzen mittels einer Template--Sprache von einem \ac{MPS} \ac{AST} in einen anderen \ac{MPS} \ac{AST}.

\begin{wrapfigure}{lH}{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.48\textwidth]{../assets/img/diagrams/mps.mmd}
    \end{center}
    \caption{\acs{MPS} \enquote{Compiler--Pipeline}}
    \label{fig:mps-compiler-pipeline}
\end{wrapfigure}

So werden verschiedene Sprachen kaskadiert, bis jedes verbleibende Node im \ac{AST} einen zugehörigen TextGen aspect hat.
Dies hat den Vorteil, dass Compiler wiederverwendet werden können und sich durch Emergenz komplexe Systeme ergeben.

Da kein Text bearbeitet wird ist auch die Entwicklererfahrung eine andere.
Es wird nicht nur die Compiler--Pipeleine beschrieben, sondern auch die zugehörigen \acs{IDE}--Features.
Dies bietet dem Sprachdesigner mehr freiheit um die Entwicklererfahrung zu optimieren.
Diese Features sind aber alle nicht notwendig, um eine funktionierende Sprache zu erstellen.

\subsubsection{BaseLanguage}
\ac{MPS} Sprachen zielen meistens darauf ab, Transpiler zu existierenden \ac{GPL} oder \ac{DSL} zu sein.
Deshalb sind \ac{XML} und Java bereits als Sprachen mit einem vorhandenem TextGen Aspect implementiert.

\ac{MPS} selbst basiert auf Java.
Deshalb wird die vorhandene Java implementierung auch als BaseLanguage bezeichnet.

Alle Aspekte (siehe \autoref{subsubsec:aspekte}) definieren ihre eigene Sprache basierend auf BaseLanguage.
Das heißt, auch beim beschreiben von Logik für TextGen, Migrationen, Generatoren, etc.\ wird BaseLanguage verwendet.
Alle Aspekte sind gute Beispiele für Emergenz in \ac{MPS}.

\subsubsection{Aufbau von \acs{MPS}}
JetBrains IntelliJ basierte \acp{IDE}, wie \ac{MPS}, untergliedern Projekte in Module.~\autocite{jetbrains-sro-no-dateD}
Ein projekt hat dabei ein oder mehrere Module.
Ein Modul ist dabei immer etwas, das kompiliert werden kann.
So können in einem \ac{IDE} Fenster / Projekt / Monorepo mehrere Programmiersprachen / Frameworks mit unterschiedlichen Compiler--Pipelines hausen.

\ac{MPS} kennt dabei primär zwei arten von Modulen:
Sprachen (language) und Lösungen (solution.)
Sprachen beschreiben dabei im weitesten Sinne eine Compiler--Pipeline und Lösungen sind Programme in diesen Sprachen.

Auch zu erwähnen sind DevKits, welche mehrere Sprachen und Lösungen gruppieren können.

\paragraph{Sprachen} setzen sich aus verschiedenen Aspekten zusammen.
Aber auch zugehörige Lösungen, wie der Generator gehören zu der Sprache.

\paragraph{Lösungen} sind untergliedert in Models.~\autocite{jetbrains-sro-no-dateA}
Models sind organisatorische Einheiten, die Code beinhalten.
Sie können Dependencies auf andere Models und verwendete Sprachen spezifizieren.

In Models können nun instanzen von \ac{AST} Nodes erstellt werden, die von einer verwendeten Sprache als \textit {rootable} ($S$) markiert wurden.
Diese können dann wie sonst Dateien geöffnet und damit über den Editor Aspekt bearbeitet werden.

Außerdem können \enquote{Dateien} in \enquote{Ordner} untergliedert werden.
Dies nennt sich dann virtuelles Package und ist in wirklichkeit teil des \enquote{Dateinamens}.

\subsubsection{Aspekte}\label{subsubsec:aspekte}
Anstelle einer Compiler--Pipeline, wie in \autoref{fig:mps-compiler-pipeline} suggeriert, sind \ac{MPS} Sprachen untergliedert in verschiedene Aspekte.
Grundlegend stimmt der Fluss aus \autoref{fig:mps-compiler-pipeline}, allerdings passieren einige Aspekte an den Pfeilen.

Durch das untergliedern der Sprachen in Aspekte erzielt \ac{MPS} \ac{SoC}.
Die meisten Aspekte sind optional.
Der Overhead ist also minimal.

\paragraph{Structure} beschreibt den \ac{AST} der Sprache.
Die Nodes des \ac{AST} werden dabei durch Konzepte beschrieben.
Nodes sind also serialisierungen von Konzepten, so wie in der \ac{OOP} Objekte serialisierungen von Klassen sind.

\lstinputlisting[label={lst:concept-structure},caption={Aufbau eines Concepts},language=mps-concept]{../assets/code/mps/structure.concept}
Konzepte sind dabei weitestgehend analog zu \ac{CFG} aufgebaut.

\subparagraph*{}
Spezifischer, mehrere Grammatiken, da es mehrere Startsymbole ($S$) geben kann.
Ein Konzept ist ein Startsymbol, wenn $\verb|instance can be root|=\verb|true|$.
Technisch gesehen arbeitet \ac{MPS} also nicht mit einem einzelnen \ac{AST}, sondern mit einem Wald von \acp{AST}.
Jedes Root--Node produziert dabei seine eigene Datei\footnote{Es können im Generator allerdings auch noch mehrere Roots kombiniert werden oder neue Roots erzeugt werden. Die Regel ist nur für TextGen auch eine explizite Regel.}.

\subparagraph*{}
\verb|properties| sind die Terminale ($\Sigma$) der \ac{CFG}.
Sie werden in \ac{MPS} as primitive Typen bezeichnet.
Sie umfassen allerdings nicht von Haus aus alle primitiven Typen, die Java spezifiziert.
\ac{MPS} spezifiziert lediglich \verb|boolean|, \verb|integer| und \verb|string|.
Weitere primitive Typen können allerdings manuel spezifiziert werden.

\textbf{Constrained Data Types} funktionieren genau wie Lex, indem sie einen \ac{RegEx} spezifizieren.
Allerdings gibt es in \ac{MPS} keine Symbol Table.
Deshalb wird nur der Name und \ac{RegEx} spezifiziert.

\textbf{Enumerations,} oder Enums, sind lediglich eine direkte auflistung an erlaubten werten.

\subparagraph*{}
\begin{figure}
    \includegraphics[width=\textwidth]{../assets/img/diagrams/ard_player_mps_structure.mmd}
    \caption{ARD.Player MPS Sprache Struktur}
    \label{fig:ard-player-mps-structure}
\end{figure}
\verb|children| und \verb|references| sind die Nicht--Terminale ($V$) sowie Produktionsregeln ($R$).
Sie verweisen jeweils nicht auf primitive Typen, sondern auf andere Konzepte.

Im Klassendiagramm aus \autoref{fig:ard-player-mps-structure} sind vor allem Aggregationen zu erkennen.
Diese werden durch die \verb|children| beschrieben.
Es gibt allerdings auch Associationen.
Diese sind die \verb|references|.

\textbf{Aggregationen} haben dabei eine Kardinalität von \verb|1| (Required), \verb|0..1| (Optional), \verb|1..n| (Many) oder \verb|0..n| (Many Optional.)
Sie sind einfache Kinder im Sinne der Baum Datenstruktur und gehören damit ausschließlich zu diesem Node.

\textbf{Associationen} hingegen haben eine Kardinalität von \verb|1| (Required) oder \verb|0..1| (Optional.)
In den meisten Fällen ersetzen die \verb|references| die Symbol Table.
Dies tun sie, indem sie einen direkten Verweis auf ein beliebiges Node herstellen.

\verb|references| sind also eigentlich eine Mischung aus Terminalen und Nicht--Terminalen, da würden wir sie nur als Nicht--Terminale betrachten, der \ac{AST} kein Baum, sondern ein Graph, wäre.
Sie können als Terminale betrachtet werden, die einen Eintrag in einer nicht--existenten Symbol Table repräsentieren.

Diese spezifische Art von Einträgen bezieht sich auf verschiedene \enquote{Stacks}.
Also zum Beispiel den Call--Stack, den Variable--Stack, etc.
Effektiv alles, was ein Scope hat.

\verb|references| bieten somit einen einfachen Weg, um komplexe Sprachfeatures, wie Variablen, Methoden und Klassen umzusetzen.

\subparagraph*{}
Das letzte, wichtige Feature der Struktur als Grammatik ist die Abstraktion.
Sie ist beispielhaft an der \ac{JSON}--Sprache aus \autoref{fig:ard-player-mps-structure} zu erkennen.

Analog zu Java gibt es abstrakte Konzepte (Klasse) sowie Interfaces.
Damit wird Mehrfachvererbung umgangen, da jedes Konzept nur einen direkten Parent haben kann, aber beliebig viele Interfaces implementieren kann.

Diese Vererbung, beziehungsweise der daraus hervorgehende Polymorphismus, mach es möglich, dass eine Regel (also \verb|children| oder \verb|references|) auf mehrere Arten aufgelöst werden kann.
Und diese Auflösungsmöglichkeiten können sogar von anderen Sprachen erweitert werden\footnote{Ist dies nicht gewünscht, kann das erben von einem Konzept mit dem \textit{final} Schlüsselwort unterbunden werden.}.

Außerdem werden \acs{OOP}--Paradigmen eingebracht.
Da \ac{OOP} eines der bekanntesten Paradigmen von \ac{GPL} sind, macht es die Struktur für viele Entwickler intuitiver als \acp{CFG}.

\paragraph{Editor} beschreibt die Darstellung sowie grundlegende Modifikation des \ac{AST}.
Im Editor--Aspekt, beziehungsweise der zugehörigen Editor--Sprache\footnote{\ac{MPS} folgt dem Grundsatz des \ac{LOP}, deshalb definieren die meisten Aspekte ihre eigenen Sprache.}, gibt es deutlich mehr Konzepte als in der Structure.

Diese Konzepte untergliedern sich in zwei Arten, \textbf{Editoren} und \textbf{Transformatoren}.
Editoren zeigen dabei den \ac{AST} strukturiert an\footnote{Properties können in Editoren auch ohne weiteres bearbeitet werden und benötigen keine Transformatoren. Außerdem können Nodes erstellt und zerstört werden.} und Transformatoren beschreiben, wie der \ac{AST} bearbeitet werden kann.
Beides passiert immer pro Konzept.

Es gibt zwei Arten von Editoren:
\begin{itemize}
    \item \textbf{Reflective Editor} ist der Standardeditor.
    Ist für ein Konzept kein Custom Editor spezifiziert, so wird immer der Reflective Editor verwendet.
    Die Darstellung ist sehr nah am \ac{AST} gehalten und erlaubt es immer, alles zu editieren.
    Der Nutzer der Sprache kann sich im Zweifelsfall immer mittels eines Hotkeys den Reflective Editor anzeigen lassen.
    \item \textbf{Custom Editor}, kurz Editor, ist eine vom Sprachdesigner definierte Ansicht für ein Konzept.
    Diese ansichten werden dann ineinander geschachtelt, sodass sich eine \enquote{schöne} gesamtansicht ergibt.
    Im weiteren wird sich nur auf diese Art von Editor bezogen.
\end{itemize}

\subparagraph{Editor} hat als seine drei wichtigsten Konzepte den \verb|Concept Editor|, also den Editor für ein Konzept; die \verb|Editor Component|, eine wiederverwendbare Teildarstellung eines Konzepts und Stylesheets.
Die \enquote{Bühne} des Editors untergliedert sich dabei in zwei Bereiche:
\begin{itemize}
    \item Der klassische Datei--Editor verhält sich, wie der Texteditor jeder anderen Textbasierten Programmiersprache.
    \item Der Inspector hingegen, ist ein \ac{UI}--Panel der \ac{IDE}, welches, basierend auf der Cursor--Position, die möglichkeit bietet, meist ein einzelnes Node, mit weiteren Werten und Einstellungen zu bereichern.
    Ein Beispiel ist die Editor--Sprache selbst.
    Hier wird das Layout im Datei--Editor beschrieben, aber Stylings unt nutzerinteraktionen der einzelnen Zellen werden im Inspektor festgelegt.
\end{itemize}
Kurz gesagt gibt es also einen allgemeinen und einen, optionalen, spezifischen Editor.

Beide dieser Bereiche werden mit der selben Layout--Sprache, der Editor--Sprache, beschrieben.
Dabei wird die Position der einzelnen \verb|properties|, \verb|children| und \verb|references| festgelegt.

Dies ermöglicht es auch, Tabellen und Diagramme als Editor einzufügen.

\subparagraph{Transformator} ist in erster Linie das \verb|Transformation Menu|--Konzept.
In erster Linie werden Textbasierte Transformationen beschrieben.
Das heißt, entweder links oder rechts von einem existierenden Node oder an einer Stelle, an der ein neues Node erwartet wird, wird Text geschrieben.
Für diesen Text wird dann bei jeder Veränderung eine Methode aufgerufen, welche evaluiert, was passieren soll.
Ist der Text spezifisch genug, so wird zumeist sofort eine Änderung am \ac{AST} vollzogen.
Anderenfalls wird meist das Autocomplete--Menü um Optionen angereichert.

Des weiteren kann analog zum Autocomplete--Menü der sogenannte \textbf{Context Assistant} bereichert werden.
Dieser ist ein eigenes \acs{UI}--Panel in der \ac{IDE}.

\paragraph{Actions} beschreiben, wie Nodes sich verändern.
\lstinputlisting[label={lst:nodefactories},caption={Node Factories Beispiel},language=mps-nodefactories]{../assets/code/mps/json.nodefactories}
\verb|Node Factories| sind vor allem für die Transformatoren wichtig.
In \autoref{lst:nodefactories} wird zum beispiel gezeigt, wie immer, wenn ein \verb|JSONDouble| erstellt wird, er zu \verb|0.0| initialisiert wird.
Außerdem wird, wenn er einen \verb|JSONInteger| ersetzt, die ganzzahl übernommen.
Hier werden also neue Nodes initialisiert.

Des weiteren wird das verhalten von Copy und Paste in den \verb|Copy/Paste Handlers| beschrieben.

\paragraph{Intentions}
\lipsum[5]

\paragraph{Refactorings}
\lipsum[5]

\paragraph{FindUsages}
\lipsum[5]

\paragraph{Scripts}
\lipsum[5]

\paragraph{Constraints}
\lipsum[5]

\paragraph{Feedback} beschreibt, wie das verstoßen gegen Constraints angezeigt werden soll.
Es werden also Fehlermeldungen aus den umgebenen Nodes zusammengetragen.

\paragraph{Typesystem}
\lipsum[5]

\paragraph{DataFlow} beschreibt den Kontrollfluss einer \ac{DSL}.
Das Ziel dabei ist es, unnötigen Code zu finden.
Es ist also ein Optimizer.
Dabei wird vom \ac{AST} zu einer semantischen Darstellung abgebildet, sodass diese analyse weitestgehend automatisiert ablaufen kann.

\paragraph{TextGen} ist \ac{MPS} equivalent zu einem klassischen Compiler.
Hier werden Nodes in String umgewandelt.
Hierzu wird für jedes Konzept eine Funktion erstellt.
Diese Funktion gibt geordnete Strings und Knoten aus, sodass die \verb|children| und \verb|references| zusammen mit dem Knoten selbst dargestellt werden.

Root--Nodes müssen zusätzlich einen Namen und Pfad, relativ zum Generationsausgabeverzeichnis, spezifizieren, damit eine Datei erstellt werden kann.

\paragraph{Behavior}
\lipsum[5]

\paragraph{Plugin}
\lipsum[5]

\paragraph{Test}
\lipsum[5]

\paragraph{Migration}
\lipsum[5]

\paragraph{\ac{VCS}} beschreibt, wie Merge--Konflikte behandelt werden sollen.
Da \ac{MPS} keine Textdatei als eingabe hat, sondern der \ac{AST} als XML Datei gespeichert wird, sind klassische Tools, wie Git, zwar nicht nutzlos, allerdings kommen sie doch schnell an ihre Grenzen.
In Diesem Aspekt wird daher beschrieben, wie verschiedene Teilbäume ineinander gemerged werden können.
Dies bietet den größen Vorteil, dass ungleich bei Textbasierten Programmiersprachen, der Merge mit rücksicht auf die Syntax der Sprache stattfindet.

\subsubsection{Generator}
\lipsum[5]

\paragraph{GenPlan} beschreibt die Reihenfolge, in der verschiedene Generatoren ausgeführt werden.
Für die meisten, einfachen Sprachen erstellt \ac{MPS} automatisiert einen GenPlan.
Bei komplexen \ac{MPS}--Projekten kann ein manuel erstellter GenPlan allerdings schnell notwendig werden.

Außerdem ist es mit einem GenPlan möglich, eine Sprache in mehrere Zielsprachen zu kompilieren.
Es kann zum Beispiel ein GenPlan für \ac{JSON} und einer für \ac{XML} erstellt werden.

\paragraph{Debugging} des Generators funktioniert am einfachsten über sogenannte Transient Models.
Diese können auf Kosten der geschwindigkeit beim kompilieren aktiviert werden.
Anschließend können sich, nach jedem Compilieren, alle Zwischenergebnisse der verschiedenen Compiler angeschaut werden.
Damit lässt sich meistens der Reducer ausmachen, der für einen Fehler verantwortlich ist.

\subsubsection{Library Code}
Es gibt zwei Wege, um Libraries mit einer Sprache zu verknüpfen.

\paragraph{Accessory Model} ist ein Model, welches mit einer Sprache assoziiert wird.
Es ist also eine Möglichkeit, Nodes, die die Konzepte serialisieren mit der Sprache zu verbinden.
Dies ermöglicht es dem Nutzer der Sprache, bestimmte Strukturen vorab bereitgestellt zu bekommen.
Dies ist analog zu den sogenannten Standardbibliotheken der meisten, herkömmlichen \ac{GPL}.

\paragraph{Runtime Module,} oder auch Library genannt, sind Module, die beschreiben, wie eine bestimmte, Nicht--\acs{MPS}--Bibliothek aufgebaut ist.
Im Sinne der BaseLanguage können so zum Beispiel vorhandene Java Bibliotheken nutzbar gemacht werden.

Im falle von Java können diese mittels eines Scripts automatisiert erstellt werden.
Für andere Sprachen müssen korrespondierende Nodes manuel erstellt werden.
